model:
  backbone: timm_tf_efficientnet_b0
  ckpt_pretrained: weights/effnetb4_damselfly_stage1_scArcFace/swa
  num_classes: 4

train:
  n_epochs: &epochs 30
  amp: True # set this to True, if your GPU supports FP16. 2080Ti - okay, 1080Ti - not okay
  ema: True # optional, but I recommend it, since the training might get unstable otherwise
  ema_decay_per_epoch: 0.4 # for middle/big datasets. Increase, if you have low amount of samples
  logging_name: effnetb4_damselfly_stage2_scArcFace
  target_metric: accuracy
  stage: second # first = Supcon, second = FC finetuning for classification

dataset: /home/agporto/Dropbox/Year_2023/morph_moritz_aligned/biosup/

dataloaders:
  train_batch_size: 80 # the higher - the better
  valid_batch_size: 80
  num_workers: 16 # set this to num of threads in your CPU

optimizer:
  name: SGD
  params:
    lr: 1.52E-03

scheduler:
  name: CosineAnnealingLR
  params:
    T_max: *epochs
    eta_min: 0.000152 # Make sure it is smaller than lr

criterion:
  name: 'LabelSmoothing'
  params:
    classes: 100
    smoothing: 0.01

img_size: &size 384

augmentations:
  transforms: 
    - RandomResizedCrop:
        height: *size
        width: *size
        scale:  !!python/tuple [0.8,1]
    - Flip:
    - Rotate:
